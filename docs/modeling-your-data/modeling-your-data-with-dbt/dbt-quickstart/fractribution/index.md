---
title: "Fractribution"
sidebar_position: 105
---

## Requirements


In addition to [dbt](https://github.com/dbt-labs/dbt) being installed and a web events dataset being available in your database:

- have `snowplow_web_page_views` derived table available as a source (generated by the [snowplow_web package](/docs/modeling-your-data/modeling-your-data-with-dbt/dbt-models/dbt-web-data-model/index.md))
- have a table with revenue data by users (`domain_userid`, `user_id`) that serves as another source for the fractribution calculations, you can choose either of the following options:
    - your `atomic.events` table with any [self-describing event](/docs/understanding-tracking-design/out-of-the-box-vs-custom-events-and-entities/#self-describing-events) that captures revenue data
    - the `snowplow_ecommerce_transaction_interactions` derived table generated by the [snowplow_ecommerce](/docs/modeling-your-data/modeling-your-data-with-dbt/dbt-models/dbt-ecommerce-data-model/index.md) package
    - any custom incremental table that is built on top of the `snowplow_web` model that results in an aggregated revenue dataset



## Setup



 #### 1. Overwrite variable defaults (where necessary)

- `conversion_window_start_date`: The start date in UTC for the window of conversions to include
- `conversion_window_end_date`: The end date in UTC for the window of conversions to include
- `conversion_hosts`: `url_hosts` to consider
- `path_lookback_steps`: The limit for the number of marketing channels to look at before the conversion (default is 0 = unlimited)
- `path_lookback_days`: Restrict the model to marketing channels within this many days of the conversion (values of 30, 14 or 7 are recommended)
- `path_transforms`: An array of path transforms and their arguments (see [Path Transform Options](/docs/modeling-your-data/modeling-your-data-with-dbt/dbt-models/dbt-fractribution-data-model/index.md#path-transform-options) section)
- `consider_intrasession_channels`: Boolean. If `false`, only considers the channel at the start of the session (i.e. first page view). If `true`, considers multiple channels in the conversion session as well as historically.
- `page_views_source`: The source (schema and table) of the derived `snowplow_web_page_views` table. Defaulted to `derived.snowplow_web_page_views`.
- `conversions_source`: The source (schema and table) of the conversion event data. Defaulted to `atomic.events`.

#### 2. Configure macros

##### Configure the conversion_clause macro

 The `conversion_clause` specifies how to filter Snowplow events to only conversion events. How this is filtered will depend on your definition of a conversion. The default is filtering to events where `tr_total > 0`, but this could instead filter on `event_name = 'checkout'`, for example. If you are using the e-commerce model, you will still need to set this for the fractribution code to run (even though all events are conversions in the e-commerce model), in this case change it to `transaction_revenue > 0`.

 If you wish to change this filter, copy the `conversion_clause.sql` file from the macros folder in the `snowplow_fractribution` package (at `[dbt_project_name]/dbt_packages/snowplow_fractribution/macros/conversion_clause.sql`) and add it to the macros folder of your own dbt project. Update the filter and save the file.

##### Configure the conversion_value macro

 The `conversion_value` macro specifies either a single column or a calculated value that represents the value associated with that conversion. The default is `tr_total`, but revenue or a calculation using revenue and discount_amount from the default e-commerce schema, for example, could similarly be used.

 If you wish to change this value, copy the `conversion_value.sql` file from the macros folder in the snowplow_fractribution package (at `[dbt_project_name]/dbt_packages/snowplow_fractribution/macros/conversion_value.sql`) and add it to the macros folder of your own dbt project. Update the value and save the file.

##### Configure the default channel_classification macro

 The `channel_classification` macro is used to perform channel classifications. This can be altered to generate your expected channels if they differ from the channels generated in the default macro. It is highly recommended that you examine and configure this macro when using your own data, as the default values will not consider any custom marketing parameters.

 If you wish to change the channel classification macro, copy the `channel_classification.sql` file from the macros folder in the snowplow_fractribution package (at `[dbt_project_name]/dbt_packages/snowplow_fractribution/macros/channel_classification.sql`) and add it to the macros folder of your own dbt project. Update the SQL and save the file.

 ##### Configure the channel_spend macro

 The `channel_spend` macro is used to query the spend by channels. It requires a user supplied SQL script to extract the total ad spend by channel.

 Required output schema:
 - channel: STRING NOT NULL
 - spend: FLOAT64 (Use the same monetary units as conversion revenue, and NULL if unknown.)

If you wish to change the channel classification macro, copy the `channel_spend.sql` file from the macros folder in the snowplow_fractribution package (at `[dbt_project_name]/dbt_packages/snowplow_fractribution/macros/channel_spend.sql`) and add it to the macros folder of your own dbt project. Update the SQL and save the file.

 #### 3. Run the model

 Execute the following either through your CLI, within dbt Cloud, or within [Snowplow BDP](/docs/modeling-your-data/running-data-models-via-snowplow-bdp/dbt/using-dbt/index.md)

 ```yml
 dbt run --select snowplow_fractribution
 ```
 #### 4. Run the python script to generate the final models

Python scripts and `requirements.txt` can be found at `[dbt_project_name]/dbt_packages/snowplow_fractribution/utils/`. To run the fractribution script locally in Python, we recommend using a virtual environment.

Snowpark requires Python 3.8. To use conda:

```
conda create --name fractribution_env -c https://repo.anaconda.com/pkgs/snowflake python=3.8 absl-py
conda activate fractribution_env
```
***

**M1 Instructions**

:::caution
There is an issue with running Snowpark on M1 chips. A workaround recommended by Snowflake is to set up a virtual environment that uses x86 Python:

```
CONDA_SUBDIR=osx-64 conda create -n fractribution_env python=3.8 absl-py -c https://repo.anaconda.com/pkgs/snowflake
conda activate fractribution_env
conda config --env --set subdir osx-64
```
:::
***

Install snowpark in this environment (all computers):

```
conda install snowflake-snowpark-python
```

Set the connection parameters to your Snowflake warehouse on the command line:

```
export snowflake_account=my_account\
export snowflake_user=sf_user\
export snowflake_password=password\
export snowflake_user_role=special_role\
export snowflake_warehouse=warehouse_name\
export snowflake_database=database_name\
export snowflake_schema=schema_name
```
***

Run the fractribution script by specifying the conversion window start and end dates and the attribution model (if you are not using the default (`shapely`)). Example:

```
python main_snowplow_snowflake.py --conversion_window_start_date '2022-06-03' --conversion_window_end_date '2022-08-01' --attribution_model last_touch
```

The output of the fractribution analysis will be built into the schema specified in your connection parameters. There are three tables that will be created are:
- `snowplow_fractribution_report_table`: The main output table that shows conversions, revenue, spend and ROAS per channel.
- `snowplow_fractribution_channel_attribution`: The conversion and revenue attribution per channel (used to create the report table).
- `snowplow_fractribution_path_summary_with_channels`: An intermediate table that shows, for each unique path, a summary of conversions, non conversions and revenue, as well as which channels were assigned a contribution.
