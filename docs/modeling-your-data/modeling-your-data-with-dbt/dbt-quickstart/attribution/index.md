---
sidebar_label: "Attribution"
sidebar_position: 400
title: "Attribution Quickstart"
---

```mdx-code-block
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import ThemedImage from '@theme/ThemedImage';
import Badges from '@site/src/components/Badges';
import { Accelerator } from "@site/src/components/AcceleratorAdmonitions";
```

## Requirements

In addition to [dbt](https://github.com/dbt-labs/dbt) being installed and a web events dataset being available in your database:

- have `snowplow_unified_views` table available as a path (touch points) source (generated by the [snowplow_unified package](/docs/modeling-your-data/modeling-your-data-with-dbt/dbt-models/dbt-unified-data-model/index.md))
- have `snowplow_unified_conversions` table available as a conversions source including the revenue (generated by the optional conversions module of the [snowplow_unified package](/docs/modeling-your-data/modeling-your-data-with-dbt/dbt-models/dbt-unified-data-model/index.md))
- optionally have a spend source table of your choice available which will contain your marketing spend data by channel and or campaign with a timestamp field which denotes the period. This is needed for the ROAS calculation for the drop and recompute report table

```mdx-code-block
import DbtPrivs from "@site/docs/reusable/dbt-privs/_index.md"

<DbtPrivs/>
```

```mdx-code-block
import DbtPackageInstallation from "@site/docs/reusable/dbt-package-installation/_index.md"

<DbtPackageInstallation/>
```

## Setup

### 1. Set variables

Marketing Attribution Analysis can be done in many different ways depending on your business and needs. Our aim has always been to make it flexible for users to make changes from selecting a different data source to reshaping how the customer journey to conversion will be adjusted for the analysis. We suggest taking the time initially, before running the model to understand what setup you would need in order to avoid having to do a full-refresh of the package.

Each of such variables will be explained below, you should edit these in your `dbt_project.yml` file. Further customization can be done via the variables listed in the [configuration page](/docs/modeling-your-data/modeling-your-data-with-dbt/dbt-configuration/attribution/index.mdx).

- `snowplow_attribution_start_date`: The start date in UTC for the package to run. Please bear in mind it will process all paths from the path_source (snowplow_unified_views, unless changed) since this date until the timestamp of the last conversion event in the conversions_source for the first run of the package
- `snowplow__conversion_path_source`: The source table for the path information. By default it will take values from `{{target.schema ~ '_derived'}}.snowplow_unified_views`
- `snowplow__conversions_source`: The source table for the conversions & revenue information. By default it will take values from `{{target.schema ~ '_derived'}}.snowplow_unified_conversions`
- `snowplow__conversion_hosts`: `url_hosts` to process, if left empty it will include all
- `snowplow__conversion_clause`: A user defined sql script to filter on specific conversions if needed. Defaulted to 'cv_value > 0'
- `snowplow__path_transforms`: A dictionary of path transforms and their arguments (see [Path Transform Options](/docs/modeling-your-data/modeling-your-data-with-dbt/dbt-models/dbt-attribution-data-model/index.md#reduce-the-number-of-paths-to-analyze) section)
- `snowplow__spend_source`: The optional source table for the spend information used to calculate ROAS in the `snowplow_attribution_overview`. By default it is empty and will not get processed


```yml title="dbt_project.yml"
vars:
  snowplow_attribution:
    snowplow_attribution_start_date: '2023-01-01'
    snowplow__conversion_path_source: 'my_schema_derived.snowplow_unified_views'
    snowplow__conversions_source: 'my_schema_derived.snowplow_unified_conversions'
    snowplow__conversion_hosts: ['mysite.com']
    snowplow__path_transforms: {'exposure_path' : null}
```

### 2. Configure macros

All the below macros are created with the intention to let users modify them to fit their personal use case. If you wish to change this, copy the macro from the macros folder in the `snowplow_attribution` package (e.g. at `[dbt_project_name]/dbt_packages/snowplow_attribution/macros/paths_to_conversion.sql`) and add it to the macros folder of your own dbt project where you are free to make any alterations. You will find a detailed guide / illustration with sample code within the individual macros themselves.

```mdx-code-block
import AttributionDbtMacros from "@site/docs/reusable/attribution-dbt-macros/_index.md"

<AttributionDbtMacros/>
```
### 3. Run the model

Execute the following either through your CLI, within dbt Cloud, or within [Snowplow BDP](/docs/modeling-your-data/running-data-models-via-snowplow-bdp/dbt/index.md)

```yml
dbt run --select snowplow_attribution
```
