---
title: "Fractribution"
sidebar_position: 106
---

```mdx-code-block
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import ThemedImage from '@theme/ThemedImage';
import Badges from '@site/src/components/Badges';
```

## Requirements

In addition to [dbt](https://github.com/dbt-labs/dbt) being installed and a web events dataset being available in your database:

- have `snowplow_web_page_views` derived table available as a source (generated by the [snowplow_web package](/docs/modeling-your-data/modeling-your-data-with-dbt/dbt-models/dbt-web-data-model/index.md))
- have a table with revenue data by users (`domain_userid`, `user_id`) that serves as another source for the fractribution calculations, you can choose either of the following options:
    - your `atomic.events` table with any [self-describing event](/docs/understanding-tracking-design/out-of-the-box-vs-custom-events-and-entities/#self-describing-events) that captures revenue data
    - the `snowplow_ecommerce_transaction_interactions` derived table generated by the [snowplow_ecommerce](/docs/modeling-your-data/modeling-your-data-with-dbt/dbt-models/dbt-ecommerce-data-model/index.md) package
    - any custom incremental table that is built on top of the `snowplow_web` model that results in an aggregated revenue dataset
- `python` or `docker` installed

```mdx-code-block
import DbtPackageInstallation from "@site/docs/reusable/dbt-package-installation/_index.md"

<DbtPackageInstallation/>
```

## Setup

### 1. Set variables

The package has some variables that need to be set before it can be run, you should edit these in your `dbt_project.yml` file. Further customization can be done via the variables listed in the [configuration page](/docs/modeling-your-data/modeling-your-data-with-dbt/dbt-configuration/fractribution/index.md).

- `snowplow__conversion_window_start_date`: The start date in UTC for the window of conversions to include
- `snowplow__conversion_window_end_date`: The end date in UTC for the window of conversions to include
- `snowplow__conversion_hosts`: `url_hosts` to process
- `snowplow__path_transforms`: A dictionary of path transforms and their arguments (see [Path Transform Options](/docs/modeling-your-data/modeling-your-data-with-dbt/dbt-models/dbt-fractribution-data-model/index.md#path-transform-options) section)

```yml
# dbt_project.yml
...
vars:
  snowplow_web:
    snowplow__conversion_window_start_date: '2022-01-01'
    snowplow__conversion_window_end_date: '2023-02-01
    snowplow__conversion_hosts: ['mysite.com']
    snowplow__path_transforms: {'exposure_path' : null}
```

### 2. Configure macros

All the below macros are created with the intention to let users modify them to fit their personal use case. If you wish to change this, copy the macro from the macros folder in the `snowplow_fractribution` package (at `[dbt_project_name]/dbt_packages/snowplow_fractribution/macros/conversion_clause.sql`) and add it to the macros folder of your own dbt project where you are free to make any alterations. You will find a detailed guide / illustration with sample code within the individual macros themselves.

```mdx-code-block
import FractributionDbtMacros from "@site/docs/reusable/fractribution-dbt-macros/_index.md"

<FractributionDbtMacros/>
```
### 3. Run the model

Execute the following either through your CLI, within dbt Cloud, or within [Snowplow BDP](/docs/modeling-your-data/running-data-models-via-snowplow-bdp/dbt/using-dbt/index.md)

```yml
dbt run --select snowplow_fractribution
```
### 4. Run the python script to generate the final models
<details>
<summary>Locally run python</summary>

:::tip

To run the fractribution script locally in Python, we recommend using a virtual environment such as one in `conda` or `pyenv`.

Example using conda:

```
conda create --name fractribution_env -c https://repo.anaconda.com/pkgs/snowflake python=3.8 absl-py
conda activate fractribution_env
```

:::
#### I. Install packages
You can install the packages using `pip install -r dbt_packages/snowplow_fractribution/utils/requirements.txt` (or the appropriate path from your terminal working directory).

Please note that some of the libraries are adapter specific. These are listed in the `requirements` file, and you can also find the necessary list for each adapter below:

<Tabs groupId="warehouse">
<TabItem value="bigquery" label="BigQuery" default>

- `absl-py`==`1.2.0` 
- `google-cloud-bigquery`==`3.5.0`

</TabItem>
<TabItem value="databricks" label="Databricks">

- `absl-py`==`1.2.0`, 
- `databricks-sql-connector`==`2.1.0` 
- `pandas` 

</TabItem>
<TabItem value="snowflake" label="Snowflake">

- `absl-py`==`1.2.0`, 
- `snowflake-snowpark-python`==`0.11.0` 

</TabItem>
</Tabs>




<details>
<summary>M1 Instructions (for Snowflake only)</summary>
:::caution
There is an issue with running Snowpark on M1 chips. A workaround recommended by Snowflake is to set up a virtual environment that uses x86 Python:

```
CONDA_SUBDIR=osx-64 conda create -n fractribution_env python=3.8 absl-py -c https://repo.anaconda.com/pkgs/snowflake
conda activate fractribution_env
conda config --env --set subdir osx-64
```
:::
</details>


#### II. Set the connection parameters in your terminal

<Tabs groupId="warehouse">
<TabItem value="bigquery" label="BigQuery" default>

```
export project_id=project_id\
export bigquery_dataset=bigquery_dataset\
export google_application_credentials=google_application_credentials
```

</TabItem>
<TabItem value="databricks" label="Databricks">

```
export databricks_schema=derived_schema_name\
export databricks_server_hostname=hostname\
export databricks_http_path=http_path\
export databricks_token=token
```

</TabItem>
<TabItem value="snowflake" label="Snowflake">

```
export snowflake_account=my_account\
export snowflake_user=sf_user\
export snowflake_password=password\
export snowflake_user_role=special_role\
export snowflake_warehouse=warehouse_name\
export snowflake_database=database_name\
export snowflake_schema=derived_schema_name
```

</TabItem>
</Tabs>


#### III. Run the fractribution script
Run the adapter specific main fractribution script by specifying the conversion window start and end dates, and the attribution model (if you are not using the default `shapley`, see [here](/docs/modeling-your-data/modeling-your-data-with-dbt/dbt-models/dbt-fractribution-data-model/index.md#attribution-models) for more options). Example:


<Tabs groupId="warehouse">
<TabItem value="bigquery" label="BigQuery" default>

```
python main_snowplow_bigquery.py --conversion_window_start_date '2022-06-03' --conversion_window_end_date '2022-08-01' --attribution_model last_touch
```

</TabItem>
<TabItem value="databricks" label="Databricks">

```
python main_snowplow_databricks.py --conversion_window_start_date '2022-06-03' --conversion_window_end_date '2022-08-01' --attribution_model last_touch
```

</TabItem>
<TabItem value="snowflake" label="Snowflake">

```
python main_snowplow_snowflake.py --conversion_window_start_date '2022-06-03' --conversion_window_end_date '2022-08-01' --attribution_model last_touch
```

</TabItem>
</Tabs>

</details>

<details>
<summary>Run python with docker</summary>

#### I. Pull the docker image â€‹<Badges badgeType="Docker Pulls" repo="snowplow/fractribution"></Badges>  

You can pull the docker image from Docker Hub: `docker pull snowplow/fractribution`

#### II. Set the environment variables

Add the necessary environment variables to an environment file, e.g. `configs.env`. The necessary variables will differ depending on the data warehouse you are using. The easiest way to determine the variables you need to set is to check the Dockerfile in the fractribution dbt package: `dbt-snowplow-fractribution/utils/Dockerfile`.  

Below is an example of the `config.env` file (set up for Snowflake). You do not need to specify the attribution model if using the default, `shapley`:
```
snowflake_account=youraccount.ap-southeast-2
snowflake_user=user
snowflake_password=abc123
snowflake_user_role=DBT
snowflake_warehouse=WH
snowflake_database=snowplow
snowflake_schema=FRACTRIBUTION_DERIVED

conversion_window_start_date=2022-06-03
conversion_window_end_date=2022-08-01
attribution_model=last_touch
warehouse=snowflake
```

#### III. Run the docker container

Run the docker container : 

If you are using Bigquery, 


<Tabs groupId="warehouse">
<TabItem value="bigquery" label="BigQuery" default>

With BigQuery you need to mount your service account keyfile when running the docker image
```
docker run --rm --env-file /path/to/env/file/configs.env -v /path/to/yourkeyfile.json:/keyfile.json -it snowplow/fractribution
```

</TabItem>
<TabItem value="databricks" label="Databricks">

```
docker run --rm --env-file /path/to/env/file/configs.env -it snowplow/fractribution
```

</TabItem>
<TabItem value="snowflake" label="Snowflake">

```
docker run --rm --env-file /path/to/env/file/configs.env -it snowplow/fractribution
```

</TabItem>
</Tabs>



</details>
