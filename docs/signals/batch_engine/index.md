---
title: "Batch Engine"
sidebar_position: 40
description: "In depth explanation on how the Batch Engine works."
sidebar_label: "Batch Engine"
---

While many `Attributes` can be computed in stream, those that have to be calculated over a longer period (e.g. a day or more), can only be created "offline" in the warehouse. We call them `Batch Attributes`. The entity here is typically the user, which may be the `domain_userid` or other Snowplow identifier fields, such as the logged in `user_id`. 

Examples of `Batch Attributes` are typically:
- customer lifetime values
- specific transactions that have or have not happened in the last X number of days
- first or last events a specific user generated or any properties associated to these events

## Let us do the heavy lifting
You may already have tables in your warehouse that you want to use (in which case you only have to register them as a batch source to be used by Signals), but if you don't have them, and don't want to build complex data models to do this efficiently for you. All of this to avoid recalculating the values each time over a very large table, which over time may easily be impossible. We have developed a tool to generate these `Attributes` for you, with the help of a few CLI commands, we call this the **`Batch Engine`**.

## How it works
In short, what you need to do first is to define a set of `Attributes` and register them as a View through the Python Signals SDK. Then you can use the optional CLI functionality of the SDK to generate a dbt project which will ultimately produce a view-specific attribute table. Then all that's left is to materialize the table, which will mean that Signals will regularly fetch the values from your warehouse table and sends it through the pipeline.

## Generating the dbt project
Here we assume you already defined your Views related to custom Batch Attributes, you want the Batch Engine to help generate for you. 
:::info 
Make sure you specified `offline = True` in your View definition.
:::

All you have to do is navigate to the Github repository where you want your dbt project(s) to be generated, install the Snowplow Signals SDK, set up your environment, and execute a few CLI commands (init, generate).

For a step-by-step tutorial to get you started, please check out our tutorial [here](/tutorials/snowplow_signals_cli_tutorial/intro).

## Understanding the autogenerated data models
Independently of existing Snowplow dbt packages, the Signals Batch Engine generated dbt models process the events uniquely, specifically for the purpose of the Attribute generation to help you save from unnecessary processing. In each incremental run, only the data that has been loaded since the last time the data models ran get processed, deduplicated, and only the relevant events and properties that are part of the Attribute definition (defined in the same View) are used to create a filtered_events_table. Upon successful run, the `snowplow_incremental_manifest` is updated to keep records of where each run left off.

There is a second layer of incremental processing logic dictated by the `daily_aggregation_manifest` table. After the `filtered_events` table is created or updated, the `daily_aggregates` table gets updated with the help of this manifest. It is needed due to late arriving data, which may mean that some days will need to be reprocessed as a whole. For optimization purposes there are variables to fine-tune how this works such as the `snowplow__reprocess_days` and the `snowplow__min_rows_to_process`.

Finally, the `Attributes` table is generated which is a drop and recompute table, fully updated each time an incremental update runs. This is made possible without much effort as the data is already pre-aggregated on a daily level.
![](../images/batch_engine_data_models.png)

## Variables

```yml title="dbt_project.yml"
snowplow__start_date: '2025-01-01' # date from where it starts looking for events based on both load and derived_tstamp
snowplow__app_id: [] # already gets applied in base_events_this_run
snowplow__backfill_limit_days: 1 # limit backfill increments for the filtered_events_table
snowplow__late_event_lookback_days: 5 # the number of days to allow for late arriving data to be reprocessed fully in the daily aggregate table
snowplow__min_late_events_to_process: 1 # the number of total daily events that have been skipped in previous runs, if it falls within the late_event_lookback_days, if the treshold is reached, those events will be processed in the daily aggregate model
snowplow__allow_refresh: false # if true, the snowplow_incremental_manifest will be dropped when running with a --full-refresh flag
snowplow__dev_target_name: dev
snowplow__databricks_catalog: "hive_metastore"
snowplow__atomic_schema: 'atomic' # Only set if not using 'atomic' schema for Snowplow events data
snowplow__database: # Only set if not using target.database for Snowplow events data -- WILL BE IGNORED FOR DATABRICKS
snowplow__events_table: "events" # Only set if not using 'events' table for Snowplow events data
```
