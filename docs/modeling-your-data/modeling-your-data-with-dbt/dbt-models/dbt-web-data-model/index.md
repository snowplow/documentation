---
title: "Web"
sidebar_position: 101
hide_title: true
---

```mdx-code-block
import Badges from '@site/src/components/Badges';
import TabItem from '@theme/TabItem';
import ThemedImage from '@theme/ThemedImage';
```
<Badges badgeType="dbt-package Release" pkg="web"></Badges>

# Snowplow Web Package

**The package source code can be found in the [snowplow/dbt-snowplow-web repo](https://github.com/snowplow/dbt-snowplow-web), and the docs for the [model design here](https://snowplow.github.io/dbt-snowplow-web/#!/overview/snowplow_web).**

The package contains a fully incremental model that transforms raw web event data generated by the [Snowplow JavaScript tracker](/docs/collecting-data/collecting-from-own-applications/javascript-trackers/index.md) into a series of derived tables of varying levels of aggregation.

The Snowplow web data model aggregates Snowplow's out of the box page view and page ping events to create a set of derived tables - page views, sessions and users - that contain many useful dimensions as well as calculated measures such as time engaged and scroll depth.

<p align="center">
<ThemedImage 
alt='Web Package data flow'
sources={{
light: require('./images/web-process-light.drawio.png').default, 
dark: require('./images/web-process-dark.drawio.png').default
}}
/>
</p>

## Overview

This model consists of a series of modules, each producing a table which serves as the input to the next module. The 'standard' modules are:

- Base: Performs the incremental logic, outputting the table `snowplow_web_base_events_this_run` which contains a de-duped data set of all events required for the current run of the model.
- Page Views: Aggregates event level data to a page view level, `page_view_id`, outputting the table `snowplow_web_page_views`.
- Sessions: Aggregates page view level data to a session level, `domain_sessionid`, outputting the table `snowplow_web_sessions`.
- Users: Aggregates session level data to a users level, `domain_userid`, outputting the table `snowplow_web_users`.
- User Mapping: Provides a mapping between user identifiers, `domain_userid` and `user_id`, outputting the table `snowplow_web_user_mapping`. This can be used for session stitching.

## Optional Modules

### Consent Tracking Custom Module

This custom module is built as an extension of the [dbt-snowplow-web package](/docs/modeling-your-data/modeling-your-data-with-dbt/dbt-models/dbt-web-data-model/index.md), it transforms raw `consent_preferences` and `cmp_visible` event data into derived tables for easier querying. These events are generated by the **Enhanced Consent plugin** of the [JavaScript tracker](/docs/collecting-data/collecting-from-own-applications/javascript-trackers/index.md).

:::info Important
For the incremental logic to work within the module you **must** use at least `RDB Loader v4.0.0`, as the custom module relies on the additional `load_tstamp` field for dbt native incrementalisation.

Whenever a new consent version is added to be tracked, the model expects an `allow_all` event in order to attribute the events to the full list of latest consent scopes. It is advisable to send a test event of that kind straight after deployment so that the model can process the data accurately.
:::

To enable this optional module, the web package must be correctly configured. Please refer to the [snowplow-web dbt quickstart guide](/docs/modeling-your-data/modeling-your-data-with-dbt/dbt-quickstart/web/index.md) for a full breakdown of how to set it up.

### Overview

This custom module consists of a series of dbt models which produce the following aggregated models from the raw consent tracking events:

  - `snowplow_web_consent_log`: Snowplow incremental table showing the audit trail of consent and Consent Management Platform (cmp) events

  - `snowplow_web_consent_users`: Incremental table of user consent tracking stats

  - `snowplow_web_consent_totals`: Summary of the latest consent status, per consent version

  - `snowplow_web_consent_scope_status`: Aggregate of current number of users consented to each consent scope

  - `snowplow_web_cmp_stats`: Used for modeling cmp_visible events and related metrics

  - `snowplow_web_consent_versions`: Incremental table used to keep track of each consent version and its validity


### Operation

It is assumed that the dbt_snowplow_web package is already installed and configured as per the [Quick Start](/docs/modeling-your-data/modeling-your-data-with-dbt/dbt-quickstart/index.md) instructions.

#### Enable the module

To enable the custom module simply copy the following code snippet to your own dbt_project.yml file:

```yml
# dbt_project.yml

models:
  snowplow_web:
    optional_modules:
      consent:
        enabled: true
        +schema: "derived"
        +tags: ["snowplow_web_incremental", "derived"]
        scratch:
          +schema: "scratch"
          +tags: "scratch"
          bigquery:
            enabled: "{{ target.type == 'bigquery' | as_bool() }}"
          databricks:
            enabled: "{{ target.type in ['databricks', 'spark'] | as_bool() }}"
          default:
            enabled: "{{ target.type in ['redshift', 'postgres'] | as_bool() }}"
          snowflake:
            enabled: "{{ target.type == 'snowflake' | as_bool() }}"
```

####  Run the module
If you have previously run the web model without this optional module enabled, you can simply enable the module and run `dbt run --selector snowplow_web` as many times as needed for this module to catch up with your other data. If you only wish to process this from a specific date, be sure to change your `snowplow__start_date`, or refer to the [Custom module](/docs/modeling-your-data/modeling-your-data-with-dbt/dbt-custom-models/index.md) section for a detailed guide on how to achieve this the most efficient way.

If you haven't run the web package before, then you can run it using `dbt run --selector snowplow_web` either through your CLI, within dbt Cloud, or for Enterprise customers you can use the BDP console. In this situation, all models will start in-sync as no events have been processed.

## Stray Page Pings
Stray Page Pings are pings within a session that do not have a corresponding `page_view` event within **the same session**. The most common cause of these is someone returning to a tab after their session has timed out but not refreshing the page. The `page_view` event exists in some other session, but there is no guarantee that both these sessions will be processed in the same run, which could lead to different results. Depending on your site content and user behavior the prevalence of sessions with stray page pings could vary greatly. For example with long-form content we have seen around 10% of all sessions contain only stray page pings (i.e. no `page_view` events).

We take different approaches to adjust for these stray pings at the page view and sessions levels, which can lead to differences between the two tables, but each is as accurate as we can currently make it.

### Sessions
As all our processing ensures full sessions are reprocessed, our sessions level table includes all stray page ping events, as well as all other view and ping events. We adjust the start time down based on your minimum visit length if the session starts with a page ping, and we include sessions that contain only (stray) pings. We also count page views based on the number of unique `page_view_ids` you have (from the `web_page` context) rather than using absolute `page_view` events to include these stray pings, and account for stray pings in the engaged time. Overall this is a more accurate view of a session and treats the stray pings as if they had a corresponding `page_view` event in the same session, even when they did not. 

The result of this is you may see misalignment between sessions and if you tried to recalculate them based directly off the page views table; this is because we discard stray pings during page view processing as discussed below, so the values (`page_views`, `engaged_time_in_s`, and `absolute_time_in_s`) in the sessions table may be higher, but are more accurate at a session level.

<p align="center">
<ThemedImage 
alt='Stray page ping sessionisation'
sources={{
light: require('./images/stray_sessions_light.drawio.png').default, 
dark: require('./images/stray_sessions_dark.drawio.png').default
}}
/>
</p>


### Page Views
For page views, because we cannot guarantee the sessions with the `page_view` event and all subsequent `page_ping` events are processed within the same run, we choose to discard all stray page pings. Without doing this it could be possible that you would get different results from different run configurations.

<div style ={{overflow:'hidden'}}>
<div style={{float: 'left', width: '45%'}}>
<p align="center"><strong>Without enforcing within-session view</strong></p>
<ThemedImage 
alt='Stray page ping page views'
sources={{
light: require('./images/stray_views_old-light.drawio.png').default, 
dark: require('./images/stray_views_old-dark.drawio.png').default
}}
/>

</div>
<div style={{float: 'right', width: '45%'}}>
<p align="center"><strong>With enforcing within-session view</strong></p>
<ThemedImage 
alt='Stray page ping page views'
sources={{
light: require('./images/stray_views_new-light.drawio.png').default, 
dark: require('./images/stray_views_new-dark.drawio.png').default
}}
/>
</div>
</div>

<br></br>

:::info

Currently we do not process these discarded stray page pings in any way, meaning that engaged time and scroll depth in these cases may be under representative of the true value. Due to session level reprocessing this remains a complicated issue to resolve, but please [let us know](https://github.com/snowplow/dbt-snowplow-web/issues) if you would like to help solve this!

:::
