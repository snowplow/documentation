---
title: "Advanced Usage of Snowplow dbt models"
date: "2022-10-05"
sidebar_position: 999
---
```mdx-code-block
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
```


## Incremental Logic

The general principle behind an incremental model is to identify new events/rows since the previous run of the model, and then only process these new events. This minimizes cost and reduces run times.

For mobile and web event data we typically consider a session to be a complete 'visit' and as such calculate metrics across the entire session. This means that when we have a new event for a previously processed session, we have to reprocess all historic events for that session as well as the new events. The logic followed is:

1. Identify new events since the previous run of the package.
2. Identify the `session_id` associated with the new events.
3. Look back over the events table to find all events associated with these `sessions_id`.
4. Run all these events through the page/screen views, sessions and users modules.

Given the large nature of event tables, Step 3 can be an expensive operation. To minimize cost ideally we want to:

- Know when any given session started. This would allow us to limit scans on the events table when looking back for previous events.
  - This is achieved by the `snowplow_web/mobile_base_sessions_lifecycle_manifest` model, which records the start and end timestamp of all sessions.
- Limit the maximum allowed session length. Sessions generated by bots can persist for years. This would mean scanning years of data every run of the package.
  - For the web package this is achieved by the `snowplow_web_base_quarantined_sessions` model, which stores the `session_id` of any sessions that have exceeded the max allowed session length (`snowplow__max_session_days`). For such sessions, all events are processed up until the max allowed length. Moving forward, no more data is processed for that session. This is not required for mobile.

### The Incremental Manifest

The web and mobile packages use centralized manifest tables, `snowplow_web/mobile_incremental_manifest`, to record what events have already been processed and by which model/node. This allows for easy identification of what events to process in subsequent runs of the package. The manifest table is updated as part of an `on-run-end` hook, which calls the `snowplow_incremental_post_hook()` macro.

<Tabs groupId="dbt-packages">
<TabItem value="web" label="Snowplow Web" default>

Example `snowplow_web_incremental_manifest`:

| model                            | last_success |
|----------------------------------|--------------|
| snowplow_web_page_views_this_run | '2021-06-03' |
| snowplow_web_page_views          | '2021-06-03' |
| snowplow_web_sessions            | '2021-06-02' |

</TabItem>
<TabItem value="mobile" label="Snowplow Mobile">

Example `snowplow_mobile_incremental_manifest`:

| model                                 | last_success |
| ------------------------------------- | ------------ |
| snowplow_mobile_screen_views_this_run | '2021-06-03' |
| snowplow_mobile_screen_views          | '2021-06-03' |
| snowplow_mobile_sessions              | '2021-06-02' |

</TabItem>
</Tabs>


### Identification of events to process

<Tabs groupId="dbt-packages">
<TabItem value="web" label="Snowplow Web" default>

The identification of which events to process is performed by the `get_run_limits` macro which is called in the `snowplow_web_base_new_event_limits` model. This macro uses the metadata recorded in `snowplow_web_incremental_manifest` to determine the correct events to process next based on the current state of the Snowplow dbt Web model. The selection of these events is done by specifying a range of `collector_tstamp`'s to process, between `lower_limit` and `upper_limit`. The calculation of these limits is as follows.


First we query `snowplow_web_incremental_manifest`, filtering for all enabled models tagged with `snowplow_web_incremental` within your dbt project:

```sql
select 
    min(last_success) as min_last_success,
    max(last_success) as max_last_success,
    coalesce(count(*), 0) as models
from snowplow_web_incremental_manifest
where model in (array_of_snowplow_tagged_enabled_models)
```
</TabItem>
<TabItem value="mobile" label="Snowplow Mobile">

The identification of which events to process is performed by the `get_run_limits` macro which is called in the `snowplow_mobile_base_new_event_limits` model. This macro uses the metadata recorded in `snowplow_mobile_incremental_manifest` to determine the correct events to process next based on the current state of the Snowplow dbt Mobile model. The selection of these events is done by specifying a range of `collector_tstamp`'s to process, between `lower_limit` and `upper_limit`. The calculation of these limits is as follows.

First we query `snowplow_mobile_incremental_manifest`, filtering for all enabled models tagged with `snowplow_mobile_incremental` within your dbt project:

```sql
select 
    min(last_success) as min_last_success,
    max(last_success) as max_last_success,
    coalesce(count(*), 0) as models
from snowplow_mobile_incremental_manifest
where model in (array_of_snowplow_tagged_enabled_models)
```

</TabItem>
</Tabs>

------

Based on the results the web model enters 1 of 4 states:

:::tip

In all states the `upper_limit` is limited by the `snowplow__backfill_limit_days` variable. This protects against back-fills with many rows causing very long run times.

:::
#### State 1: First run of the package

The query returns `models = 0` indicating that no models exist in the manifest.

**`lower_limit`**: `snowplow__start_date`  
**`upper_limit`**: `least(current_tstamp, snowplow__start_date + snowplow__backfill_limit_days)`

#### State 2: New model introduced

`models < size(array_of_snowplow_tagged_enabled_models)` and therefore a new model, tagged with `snowplow_web_incremental`, has been added since the last run. The package will replay all previously processed events in order to back-fill the new model.

**`lower_limit`**: `snowplow__start_date`  
**`upper_limit`**: `least(max_last_success, snowplow__start_date + snowplow__backfill_limit_days)`

#### State 3: Models out of sync

`min_last_success < max_last_success` and therefore the tagged models are out of sync, for example due to a particular model failing to execute successfully during the previous run. The package will attempt to sync all models.

**`lower_limit`**: `min_last_success - snowplow__lookback_window_hours`  
**`upper_limit`**: `least(max_last_success, min_last_success + snowplow__backfill_limit_days)`

#### State 4: Standard run

If none of the above criteria are met, then we consider it a 'standard run' and we carry on from the last processed event.

**`lower_limit`**: `max_last_success - snowplow__lookback_window_hours`  
**`upper_limit`**: `least(current_tstamp, max_last_success + snowplow__backfill_limit_days)`


#### How to identify the current state

<Tabs groupId="dbt-packages">
<TabItem value="web" label="Snowplow Web" default>

If you want to check the current state of the web model, run the `snowplow_web_base_new_event_limits` model. This will log the current state to the CLI while causing no disruption to the incremental processing of events.

```bash
dbt run --select snowplow_web_base_new_event_limits
...
00:26:28 | 1 of 1 START table model scratch.snowplow_web_base_new_event_limits.. [RUN]
00:26:29 + Snowplow: Standard incremental run
00:26:29 + Snowplow: Processing data between 2021-01-05 17:59:32 and 2021-01-07 23:59:32
```

</TabItem>
<TabItem value="mobile" label="Snowplow Mobile">

If you want to check the current state of the mobile model, run the `snowplow_mobile_base_new_event_limits` model. This will log the current state to the CLI while causing no disruption to the incremental processing of events.

```bash
dbt run --select snowplow_mobile_base_new_event_limits
...
00:26:28 | 1 of 1 START table model scratch.snowplow_mobile_base_new_event_limits.. [RUN]
00:26:29 + Snowplow: Standard incremental run
00:26:29 + Snowplow: Processing data between 2021-01-05 17:59:32 and 2021-01-07 23:59:32
```

</TabItem>
</Tabs>

------


## Incremental Materialization

This package makes use of the `snowplow_incremental` materialization from the `snowplow_utils` package for the incremental models. This builds upon the out-of-the-box incremental materialization provided by dbt. Its key advantage is that it limits table scans on the target table when updating/inserting based on the new data. This improves performance and reduces cost.

As is the case with the native incremental materialization, the strategy varies between adapters.

Please refer to the [snowplow-utils](https://github.com/snowplow/dbt-snowplow-utils) docs for the full documentation on `snowplow_incremental` materialization.

#### Usage Notes

- If using this the `snowplow_incremental` materialization, the native dbt `is_incremental()` macro will not recognize the model as incremental. Please use the `snowplow_utils.snowplow_is_incremental()` macro instead, which operates in the same way.
- If you would rather use an alternative incremental materialization for all incremental models within the package, set the variable `snowplow__incremental_materialization` to your preferred materialization. See the [Configuration](/docs/modeling-your-data/modeling-your-data-with-dbt/dbt-configuration/index.md) section for more details.


## Duplicates

The web and mobile packages performs de-duplication on both `event_id`'s and `page/screen_view_id`'s, in the base and page/screen views modules respectively. The de-duplication method for Redshift & Postgres is different to BigQuery, Snowflake, & Databricks due to their federated table design. The key difference between the two methodologies is that for Redshift and Postgres an `event_id` may be removed entirely during de-duplication, where as for BigQuery & Snowflake we keep all `event_id`'s. See below for a detailed explanation.

#### Redshift & Postgres
Using `event_id` de-duplication as an example, for duplicates we:

- Keep the first row per `event_id` ordered by `collector_tstamp` i.e. the earliest occurring row.
- If there are multiple rows with the same `collector_tstamp`, *we discard the event all together*. This is done to avoid 1:many joins when joining on context tables such as the page view context.

The same methodology is applied to `page/screen_view_id`s, however we order by `derived_tstamp`.

#### BigQuery, Snowflake, & Databricks

Using `event_id` de-duplication as an example, for duplicates we:

- Keep the first row per `event_id` ordered by `collector_tstamp` i.e. the earliest occurring row.

The same methodology is applied to `page/screen_view_id`s, however we order by `derived_tstamp`.

## User Mapping

The web and mobile packages contains a User Mapping module that aims to link user identifiers, namely `domain_userid`/`device_user_id` to `user_id`. The logic is to take the latest `user_id` per `domain_userid`/`device_user_id`.

The `domain_userid`/`device_user_id` is cookie/device based and therefore expires/changes over time, where as `user_id` is typically populated when a user logs in with your own internal identifier (dependent on your tracking implementation).

This mapping is applied to the sessions table by a post-hook which updates the `stitched_user_id` column with the latest mapping. If no mapping is present, the default value for `stitched_user_id`  is the `domain_userid`/`device_user_id`. This process is known as session stitching, and effectively allows you to attribute logged-in and non-logged-in sessions back to a single user.

If required, this update operation can be disabled by setting in your `dbt_project.yml` file (selecting one of web/mobile, or both, as appropriate):

<Tabs groupId="dbt-packages">
<TabItem value="web" label="Snowplow Web" default>

```yml
# dbt_project.yml
...
vars:
  snowplow_web:
    snowplow__session_stitching: false
```

</TabItem>
<TabItem value="mobile" label="Snowplow Mobile">

```yml
# dbt_project.yml
...
vars:
  snowplow_mobile:
    snowplow__session_stitching: false
```

</TabItem>
</Tabs>

------

User mapping is typically not a 'one size fits all' exercise. Depending on your tracking implementation, business needs and desired level of sophistication you may want to write bespoke logic. Please refer to this [blog post](https://snowplow.io/blog/developing-a-single-customer-view-with-snowplow/) for ideas.





## Advanced Operation

### Asynchronous Runs

You may wish to run the modules asynchronously, for instance run the screen views module hourly but the sessions and users modules daily. You would assume this could be achieved using e.g.:

```bash
dbt run --select +snowplow_mobile.screen_views
```

Currently however it is not possible during a dbt jobs start phase to deduce exactly what models are due to be executed from such a command. This means the package is unable to select the subset of models from the manifest. Instead all models from the standard and custom modules are selected from the manifest and the package will attempt to synchronize all models. This makes the above command unsuitable for asynchronous runs.

However we can leverage dbt's `ls` command in conjunction with shell substitution to explicitly state what models to run, allowing a subset of models to be selected from the manifest and thus run independently.

<Tabs groupId="dbt-packages">
<TabItem value="web" label="Snowplow Web" default>

To run just the page views module asynchronously:

```bash
dbt run --select +snowplow_web.page_views --vars "{'models_to_run': '$(dbt ls --m  +snowplow_web.page_views --output name)'}"
```

</TabItem>
<TabItem value="mobile" label="Snowplow Mobile">

To run just the screen views module asynchronously:

```bash
dbt run --select +snowplow_mobile.screen_views --vars "{'models_to_run': '$(dbt ls --m  +snowplow_mobile.screen_views --output name)'}"
```

</TabItem>
</Tabs>

------
### Cluster Keys

All the incremental models in the Snowplow packages have recommended cluster keys applied to them. Depending on your specific use case, you may want to change or disable these all together. This can be achieved by overriding the following macros with your own version within your project:

<Tabs groupId="dbt-packages">
<TabItem value="web" label="Snowplow Web" default>

- `web_cluster_by_fields_sessions_lifecycle()`
- `web_cluster_by_fields_page_views()`
- `web_cluster_by_fields_sessions()`
- `web_cluster_by_fields_users()`


</TabItem>
<TabItem value="mobile" label="Snowplow Mobile">

- `mobile_cluster_by_fields_sessions_lifecycle()`
- `mobile_cluster_by_fields_screen_views()`
- `mobile_cluster_by_fields_sessions()`
- `mobile_cluster_by_fields_users()`

</TabItem>

</Tabs>

------

### Overriding Macros

Both the cluster key macros (see above) and the `allow_refresh()` macro can be overridden. These are both [dispatched macros](https://docs.getdbt.com/reference/dbt-jinja-functions/dispatch) and can be overridden by creating your own version of the macro and setting a project level dispatch config. More details can be found in [dbt's docs](https://docs.getdbt.com/reference/dbt-jinja-functions/dispatch#overriding-package-macros).


