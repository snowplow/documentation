---
title: Automatically generated Data Models
sidebar_label: Automatically generated Data Models
sidebar_position: 8
---
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

This feature lets you generate Data Models directly from a Data Product, allowing you to automatically generate optimised, easy-to-query data models from your data product that is ready for analysis, BI-tools, or reverse ETL use cases. Analysts and Engineers are able to create data models directly from the Snowplow Console through an easy to follow workflow and use them directly in their data warehouse or integrated into a dbt project. The autogenerated data models filter for only the relevant events whilst flattening your event and entity data structures into a column for each property. We currently support Snowflake and BigQuery, with additional warehouses planned.

## Key Capabilities

### 1. Flexible Inclusion of Events and Entities
- Include all or a subset of event specifications or entities.
- Choose exactly which out-of-the-box columns to include in the model.
These two features make it possible to create data models as fine-grained and targeted as required, providing you with full control over the level of detail.

### 2. Automatic Data Flattening
To create a clean, wide table, the tool automatically flattens data structures into individual columns.

How flattening works:
- Event Data Structures → All selected event data structure properties are flattened into their own columns.
- Entities → Flattening depends on cardinality:
  - Single entities (e.g., `user`) → Flattened into separate columns, e.g., `user_id`, `user_email`.
  - Multiple entities (e.g., arrays like `products`) → Stored as a single array column, which you can unnest later.

### 3. Event Specification Inference
The system determines which events to include in the final data model through two distinct approaches:

Using Snowtype:
- Generated data models are automatically filtered to event specification IDs added by Snowtype.
- Guarantees that all events were sent with the intent to implement this Data Product.
- Ensures high data quality and tracking consistency.

Not Using Snowtype:
- Generated data models include all rows matching the Data Product definition.
- Uses "best effort" matching based on events, entities, cardinalities, and rules.
- May include some unintended events but allows access to historical data.
- Requires no adjustments to existing tracking implementations.

### 4. Deployment Options
Choose between two generation methods:

- View → Best for lightweight use cases when you need immediate access to data with minimal setup.

View statements can be run directly in your data warehouse without the need for additional tools, it is recommended when your atomic events table is smaller or you will not be regularly querying the data model. When using a View, ensure you reference the partition key of the atomic table to ensure your queries are performant and cost efficient.

- Incremental dbt Model → Ideal for production pipelines when you want to materialize data efficiently and integrate it into existing dbt projects.

Multiple types of incremental dbt models are able to be created. This allows for more efficient querying of the table and is recommended when you have a large atomic events table or will be regularly querying your modeled data.

Choose between a Simple, Normalize, or Unified incremental dbt model:

- Simple: compatible with any dbt project with no package dependencies, this model will materialize all data for the model on the first run, whilst processing only new events for every subsequent run.

- [Normalize](https://docs.snowplow.io/docs/modeling-your-data/modeling-your-data-with-dbt/dbt-models/dbt-normalize-data-model/) and [Unified](https://docs.snowplow.io/docs/modeling-your-data/modeling-your-data-with-dbt/dbt-models/dbt-unified-data-model/) models: designed to work our existing dbt packages for efficient incremental processing building upon our events_this_run tables.


| Consideration | Simple | Unified & Normalize | Views |
|---------------|--------|--------------------| ------|
| **Best for** | Standalone projects, new dbt implementations | Existing Snowplow dbt package users | Real-time data access, exploratory analysis |
| **Dependencies** | None | Requires Snowplow dbt packages | None |
| **Setup complexity** | Minimal | Moderate (variable configuration required) | Minimal |
| **Processing efficiency** | Standard incremental | Optimized with `events_this_run` tables | Query-time processing |
| **Integration** | Independent operation | Coordinated with existing Snowplow models | Independent operation |
| **Storage requirements** | Materializes data | Materializes data | No additional storage |
| **Query performance** | Fast (pre-computed) | Fast (pre-computed) | Depends on underlying data volume |
| **Data freshness** | Batch incremental updates | Batch incremental updates | Real-time |


## Instructions

### Views
Creates a dbt view that queries your atomic events table directly without materializing data, providing real-time access to transformed data with minimal storage overhead.

Steps:
1. Download the view model from the Snowplow Console.
2. Configure the model schema to decide where to create the view.
3. Run the model to create the view in your data warehouse.

### Simple
Compatible with any dbt project with no package dependencies, this model materializes all data on the first run while processing only new events for subsequent runs.

Steps:
1. Download the simple dbt model from the Snowplow Console.
2. Add the SQL file to your dbt project's models directory.
3. Run the dbt model:
	- **Initial run**: Materializes all historical data from your atomic events table.
	- **Subsequent runs**: Processes only new events since the last run timestamp.

### Unified & Normalize
Integrates with existing Snowplow dbt packages for efficient incremental processing, leveraging our events_this_run tables and established infrastructure.

Steps:
1. Download the unified dbt model from the Snowplow Console.
2. Add the SQL file to your dbt project's models directory.
3. Configure dbt project variables:
	Set `snowplow__start_date` in your dbt project variables (new models begin processing from this date since they're not yet tracked in the Snowplow Manifest table)
	Configure `snowplow__backfill_limit_days` to control the volume of data processed per run.
4. Run the dbt model iteratively until the new model catches up to your existing models (monitor progress through dbt logs and manifest updates).

## Use Cases

### Use Case 1 — Quick Data Exploration
Scenario:  
A developer needs to explore event-level data to validate tracking or analyze user behavior.

How the Feature Helps:
- Generate a lightweight View directly from a Data Product.
- Automatically flatten events and entities for easier querying.
- Select only relevant event specifications to avoid noise.

Outcome:  
Immediate access to a clean, queryable table without manual setup — enabling faster debugging and validation.

### Use Case 2 — Production-Ready Data Models
Scenario:  
A data engineer wants to integrate curated event data into an existing dbt project to power analytics dashboards.

How the Feature Helps:
- Create an incremental dbt model directly from the Data Product.
- Select only the necessary events and entities for optimal performance.
- Ensure schema consistency by aligning models with event specifications.
- Minimize integration errors and streamline analytics workflows.

Outcome:  
Seamless integration of modeled event data into production pipelines — ensuring scalability, accuracy, and maintainability.