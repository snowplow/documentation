---
title: "Creating custom models"
date: "2022-10-05"
sidebar_position: 500
---

```mdx-code-block
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
```

## Custom Modules

The Snowplow packages are designed to be easily customized or extended within your own dbt project, by building your own 'custom modules'. The 'standard modules' we provide (base, page/screen views, sessions and users) are not designed to be modified by you. An example dbt project with custom modules can be seen in the custom example directory of the [snowplow-web repo](https://github.com/snowplow/dbt-snowplow-web/tree/main/custom_example) and [snowplow-mobile repo](https://github.com/snowplow/dbt-snowplow-mobile/tree/main/custom_example).

## Guidelines & Best Practice

The Snowplow packages' modular structure allows for custom SQL modules to leverage the model's incrementalization logic, and operate as 'plugins' to compliment the standard model. This can be achieved by using the `_this_run` tables as an input, and producing custom tables which may join to the standard model's main derived tables (for example, to aggregate custom contexts to a page/screen_view level), or provide a separate level of aggregation (for example a custom user interaction).

The standard modules carry out the heavy lifting in establishing an incremental structure and providing the core logic for the most common web aggregation use cases. It also allows custom modules to be plugged in without impeding the maintenance of standard modules.

The following best practices should be followed to ensure that updates and bug fixes to the model can be rolled out with minimal complication:

- Custom modules should not modify any of the tables generated by the Snowplow packages i.e. the scratch, derived or manifest tables.
- Customizations should not modify the SQL provided by the package - they should only comprise of a new set of SQL statements, which produce a separate table.
- The logic for custom SQL should be idempotent, and restart-safe - in other words, it should be written in such a way that a failure mid-way, or a re-run of the model will not change the deterministic output.

In short, the standard modules can be treated as the source code for a distinct piece of software, and custom modules can be treated as self-maintained, additive plugins - in much the same way as a Java package may permit one to leverage public classes in their own API, and provide an entry point for custom programs to run, but will not permit one to modify the original API.

The `_this_run` and derived tables are considered part of the 'public' class of tables in this model structure, and so we can give assurances that non-breaking releases won't alter them. The other tables may be used in custom SQL, but their logic and structure may change from release to release, or they may be removed. If one does use a scratch table in custom logic, any breaking changes can be mitigated by either amending the custom logic to suit, or copying the relevant steps from an old version of the model into the custom module. _(However this will rarely be necessary)_.

## What denotes a custom module?

**Does:**  
In short, anything that plugs into the incremental framework provided by this package. Generally speaking any models you create that reference any of the `_this_run` tables from the standard modules are leveraging this framework and therefore need to be tagged with `snowplow_web/mobile_incremental` (see the [tagging](#tagging-custom-models) section). Such models will typically be materialized as incremental, although for more complex custom modules there may be a series of staging models that ultimately produce a derived incremental model. In this case, all staging models also need to be tagged correctly.

**Doesn't:**  
Models that only reference a Snowplow web derived table as their input, rather than a `_this_run` table. Since these derived tables are materialized as incremental they contain all historic events. Any models you build that reference these tables can therefore by written in a drop and recompute manner i.e. materialized as a table. This means they do not leverage the incremental framework of this package and therefore **should not be tagged.**

## Inputs for custom modules

Listed below are the recommended tables to reference as your input for a custom module, depending on the level of aggregation required:

<Tabs groupId="dbt-packages2">
<TabItem value="web" label="Snowplow Web" default>

- Event level: `snowplow_web_base_events_this_run`
- Page view level: `snowplow_web_page_views_this_run`
- Session level: `snowplow_web_sessions_this_run`
- User level: `snowplow_web_users_this_run`

</TabItem>
<TabItem value="mobile" label="Snowplow Mobile">

- Event level: `snowplow_mobile_base_events_this_run`
- Screen view level: `snowplow_mobile_screen_views_this_run`
- Session level: `snowplow_mobile_sessions_this_run`
- User level: `snowplow_mobile_users_this_run`

</TabItem>
</Tabs>

## Tagging custom models

<Tabs groupId="dbt-packages2">
<TabItem value="web" label="Snowplow Web" default>

All models within custom modules need to be tagged with `snowplow_web_incremental` in order to leverage the incremental logic of this package. We recommend creating a sub directory of your `/models` directory to contain all your custom modules. In this example we created the sub directory `snowplow_web_custom_modules`. We can then apply the tag to all models in this directory:

```yml
# dbt_project.yml
models:
  your_dbt_project:
    snowplow_web_custom_modules:
      +tags: snowplow_web_incremental #Adds tag to all models in the 'snowplow_web_custom_modules' directory
```

</TabItem>
<TabItem value="mobile" label="Snowplow Mobile">

All models within custom modules need to be tagged with `snowplow_mobile_incremental` in order to leverage the incremental logic of this package. We recommend creating a sub directory of your `/models` directory to contain all your custom modules. In this example we created the sub directory `snowplow_mobile_custom_modules`. We can then apply the tag to all models in this directory:

```yml
# dbt_project.yml
models:
  your_dbt_project:
    snowplow_mobile_custom_modules:
      +tags: snowplow_mobile_incremental #Adds tag to all models in the 'snowplow_mobile_custom_modules' directory
```

</TabItem>
</Tabs>


## Retiring Custom Modules

If you want to retire a custom module, you should:

1. Delete the models from your project or [disable](https://docs.getdbt.com/reference/resource-configs/enabled#disable-a-model-in-a-package-in-order-to-use-your-own-version-of-the-model) the models.
2. Not worry about removing the models from the `snowplow_web/mobile_incremental_manifest` manifest table. The packages identifies **enabled** models tagged with `snowplow_web/mobile_incremental` within your project and selects these models from the manifest in order to calculate the state of the web model as described above.
3. Not simply exclude the retired models from your Snowplow web job in production. Currently the packages is unable to identify which models are due to be executed in a given run. As a result, if you exclude a model the package will get stuck in State 3 as outlined in the identification of events to process section and continue to attempt to sync your excluded with the remaining models.



## Back-filling custom modules

Overtime you may wish to add custom modules to extend the functionality of this package. As you introduce new custom modules into your project, assuming they are tagged correctly (see page on [custom modules](/docs/modeling-your-data/modeling-your-data-with-dbt/dbt-custom-models/index.md)), the web and mobile models will automatically replay all events up until the latest event to have been processed by the other modules.

Note that the batch size of this back-fill is limited as outlined in the [identification of events to process](/docs/modeling-your-data/modeling-your-data-with-dbt/index.md#identification-of-events-to-process) section. This means it might take several runs to complete the back-fill, **during which time no new events will be processed by the main model**.

During back-filling, the derived page/screen views, sessions and users tables are blocked from updating. This is to protect against a batched back-fill temporarily introducing incomplete data into these derived tables.

We have created a macro `snowplow_utils.is_run_with_new_events(package_name)`, which will evaluate whether the particular model i.e. `{{ this }}` has already processed the events in the given run of the model. This is returned as a boolean and effectively blocks the upsert to incremental models if the run only contains old data. This protects against your derived incremental tables being temporarily updated with incomplete data during batched back-fills of other models. We recommend including this in the where clause of any incremental models you create within custom modules as follows:

```sql
select
  ...
from
where {{ snowplow_utils.is_run_with_new_events("snowplow_web") }}
```

A full example of this can be seen in the `custom_example` directory.

Back-filling a module can be performed either as part of the entire run of the Snowplow package, or in isolation to reduce cost (recommended):

<Tabs groupId="dbt-packages">
<TabItem value="web" label="Snowplow Web" default>

```bash
dbt run --select snowplow_web tag:snowplow_web_incremental # Will execute all Snowplow web modules, as well as custom.
dbt run --select +my_custom_module # Will execute only your custom module + any upstream nodes.
```

</TabItem>
<TabItem value="mobile" label="Snowplow Mobile">

```bash
dbt run --select snowplow_mobile tag:snowplow_mobile_incremental # Will execute all Snowplow mobile modules, as well as custom.
dbt run --select +my_custom_module # Will execute only your custom module + any upstream nodes.
```

</TabItem>
</Tabs>

------

## Tearing down and restarting a subset of models

As the code base for your custom modules evolves, you will likely need to replay events through a given module. In order to do so, you first need to manually drop the models within your custom module from your database. Then these models need to be removed from the incremental manifest table. See the [Complete refresh](#complete-refresh-of-snowplow-package) section for an explanation as to why. This removal can be achieved by passing the model's name to the `models_to_remove` var at run time. If you want to replay events through a series of dependent models, you only need to pass the name of the endmost model within the run:


<Tabs groupId="dbt-packages">
<TabItem value="web" label="Snowplow Web" default>

```bash
dbt run --select +snowplow_web_custom_incremental_model --vars '{snowplow__start_date: "yyyy-mm-dd", models_to_remove: snowplow_web_custom_incremental_model}'
```

By removing the `snowplow_web_custom_incremental_model` model from the manifest the web packages will be in state 2 (see the section on [incremental logic](/docs/modeling-your-data/modeling-your-data-with-dbt/index.md#incremental-logic)) and will replay all events.

</TabItem>
<TabItem value="mobile" label="Snowplow Mobile">

```bash
dbt run --select +snowplow_mobile_custom_incremental_model --vars '{snowplow__start_date: "yyyy-mm-dd", models_to_remove: snowplow_mobile_custom_incremental_model}'
```

By removing the `snowplow_mobile_custom_incremental_model` model from the manifest the mobile packages will be in state 2 (see the section on [incremental logic](/docs/modeling-your-data/modeling-your-data-with-dbt/index.md#incremental-logic))  and will replay all events.

</TabItem>
</Tabs>

------



## Tips for developing custom modules

While developing custom modules you may benefit from the following:

1. Minimizing the amount of data being processed to reduce cost & run time.
2. Use recent events from your events table to ensure you have all the latest contexts and event types available.
3. BigQuery only: Automatic handling of evolving schemas for custom contexts and unstructured events.

### 1. Reducing Costs

By setting `snowplow__backfill_limit_days` to 1 in your `dbt_project.yml` file you will only process a days worth of data per run.

We have provided the `get_value_by_target` macro to dynamically switch the backfill limit depending on your environment i.e. dev vs. prod, with your environment determined by your target name:

```yml
# dbt_project.yml
...
vars:
  snowplow_web:
    snowplow__backfill_limit_days: "{{ snowplow_utils.get_value_by_target(
                                            dev_value=1,
                                            default_value=30,
                                            dev_target_name='dev') }}"
```

### 2. Using Recent Data

This can be achieved by setting `snowplow__start_date` to a recent date. To dynamically change the start date depending on your environment, you can use the following:

```yml
# dbt_project.yml
...
vars:
  snowplow_web:
    snowplow__start_date: "{{ snowplow_utils.get_value_by_target(
                                      dev_value=snowplow_utils.n_timedeltas_ago(1, 'weeks'),
                                      default_value='2020-01-01',
                                      dev_target_name='dev') }}"
```

### 3. Handling of schema evolution

:::info BigQuery Only

As your schemas for such custom contexts and unstructured events evolve, multiple versions of the same column will be created in your events table e.g. `custom_context_1_0_0`, `custom_context_1_0_1`. These columns contain nested fields i.e. are of a datatype `RECORD`. When modeling Snowplow data it can be useful to combine or coalesce each nested field across all versions of the column for a continuous view over time.

The snowplow-utils package provides the [combine_column_versions](https://github.com/snowplow/dbt-snowplow-utils#combine_column_versions-source) macro, which will automatically coalesce the fields within every version of the specified column. This mitigates the need for you to update your models every time a new column version is created.

Please refer to the [snowplow-utils](https://github.com/snowplow/dbt-snowplow-utils) docs for the full documentation on these macros.

:::
